# TF-IDF原理
TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)是一种用于资讯检索与资讯探勘的常用加权技术。

TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。

总结： 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章的主题。
算法权重的设计必须满足：一个词预测主题的能力越强，权重越大，反之，权重越小。



## 词频

- 词频 (term frequency, TF) 指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否。）

- 词频公式：

​       $TF_w = \frac{在某一类中词条w出现的次数}{该类中所有的词条数目}$



## 逆向文件频率

- 逆向文件频率 (inverse document frequency, IDF) 的主要思想是：如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。

​      $IDF = log(\frac{语料库的文档总数}{包含词条w的文档数+1}), 分母之所以要加1，是为了避免分母为0$

- 某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。 

   $TF-IDF = TF * IDF$



## TF_IDF小结

TF-IDF是非常常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后我们已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，我们就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。

当然TF-IDF不光可以用于文本挖掘，在信息检索等很多领域都有使用。因此值得好好的理解这个方法的思想。



## 相关应用

[TF-IDF与余弦相似性的应用（一）：自动提取关键词](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)
[TF-IDF与余弦相似性的应用（二）：找出相似文章](http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html)



## 参考来源

https://blog.csdn.net/zrc199021/article/details/53728499

https://www.cnblogs.com/pinard/p/6693230.html



# 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。（可以使用Python中TfidfTransformer库）

# 互信息的原理

4. 使用第二步生成的特征矩阵，利用互信息进行特征筛选。

